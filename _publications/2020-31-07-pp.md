---
title: "Distributional semantic modeling: a revised technique to train term/word vector space models applying the ontology-related approach"
collection: publications
permalink: /publication/2020-31-07-pp
excerpt: 'We design a new technique for the distributional semantic modeling with a neural network-based approach to learn distributed term representations (or term embeddings) – term vector space models as a result, inspired by the recent ontology-related approach (using different types of contextual knowledge such as syntactic knowledge, terminological knowledge, semantic knowledge, etc.) to the identification of terms (term extraction) and relations between them (relation extraction) called semantic pre-processing technology – SPT. Our method relies on automatic term extraction from the natural language texts and subsequent formation of the problem-oriented or application-oriented (also deeply annotated) text corpora where the fundamental entity is the term (includes non-compositional and compositional terms). This gives us an opportunity to changeover from distributed word representations (or word embeddings) to distributed term representations (or term embeddings). This transition will allow to generate more accurate semantic maps of different subject domains (also, of relations between input terms – it is useful to explore clusters and oppositions, or to test your hypotheses about them). The semantic map can be represented as a graph using Vec2graph – a Python library for visualizing word embeddings (term embeddings in our case) as dynamic and interactive graphs. The Vec2graph library coupled with term embeddings will not only improve accuracy in solving standard NLP tasks, but also update the conventional concept of automated ontology development. The main practical result of our work is the development kit (set of toolkits represented as web service APIs and web application), which provides all necessary routines for the basic linguistic pre-processing and the semantic pre-processing of the natural language texts in Ukrainian for future training of term vector space models.'
date: 2020-08-31
venue: 'Published in scientific journal "Problemy programmirovaniâ". Preprint prepared for the International Conference of Programming UkrPROG 2020'
paperurl: 'https://doi.org/10.15407/pp2020.02-03.341'
citation: 'Palagin, O., Velychko, V., Malakhov, K., Shchurov, O. (2020). Distributional semantic modeling: a revised technique to train term/word vector space models applying the ontology-related approach. Retrieved from https://doi.org/10.15407/pp2020.02-03.341'
---

**DOI: [10.15407/pp2020.02-03.341](https://doi.org/10.15407/pp2020.02-03.341)**

### Abstract:
<p style="font-size:11pt">
We design a new technique for the distributional semantic modeling with a neural network-based approach to learn distributed term representations (or term embeddings) – term vector space models as a result, inspired by the recent ontology-related approach (using different types of contextual knowledge such as syntactic knowledge, terminological knowledge, semantic knowledge, etc.) to the identification of terms (term extraction) and relations between them (relation extraction) called semantic pre-processing technology – SPT. Our method relies on automatic term extraction from the natural language texts and subsequent formation of the problem-oriented or application-oriented (also deeply annotated) text corpora where the fundamental entity is the term (includes non-compositional and compositional terms). This gives us an opportunity to changeover from distributed word representations (or word embeddings) to distributed term representations (or term embeddings). This transition will allow to generate more accurate semantic maps of different subject domains (also, of relations between input terms – it is useful to explore clusters and oppositions, or to test your hypotheses about them). The semantic map can be represented as a graph using Vec2graph – a Python library for visualizing word embeddings (term embeddings in our case) as dynamic and interactive graphs. The Vec2graph library coupled with term embeddings will not only improve accuracy in solving standard NLP tasks, but also update the conventional concept of automated ontology development. The main practical result of our work is the development kit (set of toolkits represented as web service APIs and web application), which provides all necessary routines for the basic linguistic pre-processing and the semantic pre-processing of the natural language texts in Ukrainian for future training of term vector space models.
</p>

### Bibtex (Problemy programmirovaniâ):
```
@article{Palagin2020pp,
   title={Distributional semantic modeling: a revised technique to train term/word vector space models applying the ontology-related approach},
   ISSN={1727-4907},
   url={https://doi.org/10.15407/pp2020.02-03.341},
   number={2–3},
   journal={Problemy programmirovani{\^{a}}},
   publisher={Publishing house "Academperiodika"},
   author={Oleksandr Palagin and Vitalii Velychko and Kyrylo Malakhov and Oleksandr Shchurov},
   year={2020},
   month={March},
   pages={341–351}
}
```

### Links (full text in English):
* [arXiv.org](https://arxiv.org/a/0000-0003-3223-9844) preprint: [https://arxiv.org/abs/2003.03350](https://arxiv.org/abs/2003.03350)
* [pp.isofts.kiev.ua repository](http://pp.isofts.kiev.ua): [http://pp.isofts.kiev.ua/ojs1/article/view/426](http://pp.isofts.kiev.ua/ojs1/article/view/426)
